{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-25 18:38:55.354808: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-25 18:39:00,900 - INFO - Processing dataset: CPSC18-ALL\n",
      "2024-08-25 18:39:00,901 - INFO - Using filter combination: [32, 64, 128]\n",
      "2024-08-25 18:41:12,876 - INFO - Processing fold 1 for dataset CPSC18-ALL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "172/172 [==============================] - 1258s 7s/step - loss: 3.0978 - accuracy: 0.3279 - val_loss: 2.4534 - val_accuracy: 0.3659\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 1261s 7s/step - loss: 1.8655 - accuracy: 0.4509 - val_loss: 1.9139 - val_accuracy: 0.4147\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 1251s 7s/step - loss: 1.6366 - accuracy: 0.5039 - val_loss: 1.7984 - val_accuracy: 0.4905\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 1236s 7s/step - loss: 1.4907 - accuracy: 0.5497 - val_loss: 1.7079 - val_accuracy: 0.4657\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 1238s 7s/step - loss: 1.3721 - accuracy: 0.5695 - val_loss: 1.5645 - val_accuracy: 0.5904\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 1227s 7s/step - loss: 1.2580 - accuracy: 0.6227 - val_loss: 1.3451 - val_accuracy: 0.5583\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 1206s 7s/step - loss: 1.0831 - accuracy: 0.6674 - val_loss: 1.1964 - val_accuracy: 0.6268\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 1209s 7s/step - loss: 1.0282 - accuracy: 0.6847 - val_loss: 1.1120 - val_accuracy: 0.6523\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 1271s 7s/step - loss: 0.9310 - accuracy: 0.7089 - val_loss: 0.9826 - val_accuracy: 0.7092\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 1245s 7s/step - loss: 0.8658 - accuracy: 0.7201 - val_loss: 1.1029 - val_accuracy: 0.6633\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 1217s 7s/step - loss: 0.8168 - accuracy: 0.7383 - val_loss: 0.9824 - val_accuracy: 0.7092\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 1259s 7s/step - loss: 0.7894 - accuracy: 0.7490 - val_loss: 0.9826 - val_accuracy: 0.7179\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 1342s 8s/step - loss: 0.7477 - accuracy: 0.7534 - val_loss: 0.9381 - val_accuracy: 0.7289\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 1446s 8s/step - loss: 0.7251 - accuracy: 0.7682 - val_loss: 0.9887 - val_accuracy: 0.6902\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 1424s 8s/step - loss: 0.7015 - accuracy: 0.7722 - val_loss: 0.8740 - val_accuracy: 0.7420\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 1583s 9s/step - loss: 0.6759 - accuracy: 0.7829 - val_loss: 0.9545 - val_accuracy: 0.7354\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 1578s 9s/step - loss: 0.6604 - accuracy: 0.7902 - val_loss: 0.8882 - val_accuracy: 0.7413\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 1449s 8s/step - loss: 0.6297 - accuracy: 0.7895 - val_loss: 1.0222 - val_accuracy: 0.6888\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 1359s 8s/step - loss: 0.6075 - accuracy: 0.7986 - val_loss: 0.9114 - val_accuracy: 0.7230\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 1350s 8s/step - loss: 0.5889 - accuracy: 0.8021 - val_loss: 0.9333 - val_accuracy: 0.7165\n",
      "43/43 [==============================] - 70s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 02:04:52,413 - ERROR - An error occurred while processing CPSC18-ALL: save_results() takes 2 positional arguments but 3 were given\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Dataset definitions\n",
    "# datasets = [\n",
    "#     {\"name\": \"CUSPH-AF-AFL\", \"data\": \"data/data_2class_arrhythmia.npy\", \"labels\": \"data/labels_2class_arrhythmia.npy\", \"num_classes\": 2, \"input_shape\": (5000, 12)},\n",
    "#     {\"name\": \"CUSPH-SR-AF+AFL\", \"data\": \"data/data_2class_normal_combined.npy\", \"labels\": \"data/labels_2class_normal_combined.npy\", \"num_classes\": 2, \"input_shape\": (5000, 12)},\n",
    "#     {\"name\": \"CUSPH-ALL\", \"data\": \"data/data_11class.npy\", \"labels\": \"data/labels_11class.npy\", \"num_classes\": 11, \"input_shape\": (5000, 12)},\n",
    "#     {\"name\": \"CSPC18-SR-AF\", \"data\": \"data/data_2class_cpsc18.npy\", \"labels\": \"data/labels_2class_cpsc18.npy\", \"num_classes\": 2, \"input_shape\": (15000, 12)},\n",
    "#     {\"name\": \"CUSPH-SR-AF\", \"data\": \"data/data_2class_normal.npy\", \"labels\": \"data/labels_2class_normal.npy\", \"num_classes\": 2, \"input_shape\": (5000, 12)}\n",
    "# ]\n",
    "\n",
    "datasets = [\n",
    "#     {\"name\": \"Georgia-ALL\", \"data\": \"data/data_singleclass_georgia.npy\", \"labels\": \"data/labels_singleclass_georgia.npy\", \"num_classes\": 56, \"input_shape\": (5000, 12)},\n",
    "    {\"name\": \"CPSC18-ALL\", \"data\": \"data/data_multiclass_cpsc18.npy\", \"labels\": \"data/labels_multiclass_cpsc18.npy\", \"num_classes\": 9, \"input_shape\": (15000, 12)},\n",
    "]\n",
    "\n",
    "# datasets = [\n",
    "#     {\"name\": \"Georgia-ALL\", \"data\": \"data/data_singleclass_georgia.npy\", \"labels\": \"data/labels_singleclass_georgia.npy\", \"num_classes\": 56, \"input_shape\": (5000, 12)},\n",
    "# ]\n",
    "\n",
    "filter_combinations = [[32, 64, 128]]\n",
    "\n",
    "# def save_results(results, dataset_name, fold=None):\n",
    "#     filename = f\"results/{dataset_name}_results{'_fold_' + str(fold) if fold is not None else ''}.json\"\n",
    "#     os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "#     with open(filename, 'w') as f:\n",
    "#         json.dump(results, f, indent=2)\n",
    "\n",
    "import json\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "    \n",
    "def save_results(results, filename):\n",
    "    serializable_results = convert_to_serializable(results)\n",
    "    with open(f\"{filename}.json\", 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "def save_confusion_matrix(cm, dataset_name, fold=None):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {dataset_name}')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"results/{dataset_name}_confusion_matrix{'_fold_' + str(fold) if fold is not None else ''}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def calculate_dataset_stats(data, labels):\n",
    "    stats = {\n",
    "        'overall': {\n",
    "            'mean': np.mean(data),\n",
    "            'std': np.std(data),\n",
    "            'min': np.min(data),\n",
    "            'max': np.max(data),\n",
    "        },\n",
    "        'channel_wise': [],\n",
    "        'label_distribution': np.bincount(labels.astype(int)).tolist()\n",
    "    }\n",
    "    \n",
    "    for i in range(data.shape[-1]):\n",
    "        channel_data = data[:, :, i]\n",
    "        channel_stats = {\n",
    "            'mean': np.mean(channel_data),\n",
    "            'std': np.std(channel_data),\n",
    "            'min': np.min(channel_data),\n",
    "            'max': np.max(channel_data),\n",
    "        }\n",
    "        stats['channel_wise'].append(channel_stats)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def downsample_block(x, filters):\n",
    "    x = layers.Conv1D(filters // 2, 1, strides=1, padding='same')(x)\n",
    "    x = mixed_pool_operator(x)\n",
    "    return x\n",
    "\n",
    "def branched_nodal_operator(x, filters, kernel_size=5, activation='relu'):\n",
    "    y1 = layers.Conv1D(filters // 2, kernel_size, dilation_rate=2, padding='same')(x)\n",
    "    y1 = layers.BatchNormalization()(y1)\n",
    "    y1 = layers.Activation(activation)(y1)\n",
    "\n",
    "    y2 = layers.SeparableConv1D(filters // 2, kernel_size, padding='same')(x)\n",
    "    y2 = layers.BatchNormalization()(y2)\n",
    "    y2 = layers.Activation(activation)(y2)\n",
    "\n",
    "    y = layers.Concatenate()([y1, y2])\n",
    "    return y\n",
    "\n",
    "def mixed_pool_operator(x, pool_size=2, strides=1):\n",
    "    y1 = layers.AveragePooling1D(pool_size, strides, padding='same')(x)\n",
    "    y2 = layers.MaxPooling1D(pool_size, strides, padding='same')(x)\n",
    "    y = layers.Concatenate()([y1, y2])\n",
    "    return y\n",
    "\n",
    "def squeeze_and_excitation_block(x, ratio=16):\n",
    "    num_channels = x.shape[-1]\n",
    "    squeeze = layers.GlobalAveragePooling1D()(x)\n",
    "    excitation = layers.Dense(num_channels // ratio, activation='relu')(squeeze)\n",
    "    excitation = layers.Dense(num_channels, activation='sigmoid')(excitation)\n",
    "    excitation = layers.Reshape((1, num_channels))(excitation)\n",
    "    scale = layers.Multiply()([x, excitation])\n",
    "    return scale\n",
    "\n",
    "def residual_block_SERN_AwGOP(x, filters, kernel_size=5, downsample=False):\n",
    "    y = branched_nodal_operator(x, filters, kernel_size)\n",
    "    y = branched_nodal_operator(y, filters, kernel_size)\n",
    "\n",
    "    if downsample:\n",
    "        x = downsample_block(x, filters)\n",
    "\n",
    "    y = squeeze_and_excitation_block(y)\n",
    "\n",
    "    attention_weights = layers.Dense(1, activation='sigmoid')(x)\n",
    "    gop_out = layers.Multiply()([attention_weights, y])\n",
    "    gop_out = layers.Add()([gop_out, x])\n",
    "    gop_out = layers.Activation('relu')(gop_out)\n",
    "    return gop_out\n",
    "\n",
    "def create_SERN_AwGOP(input_shape, num_classes, filters):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(filters[0], 5, strides=2, padding='same', activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "\n",
    "    for f in filters[1:]:\n",
    "        x = residual_block_SERN_AwGOP(x, f, downsample=True)\n",
    "        x = residual_block_SERN_AwGOP(x, f)\n",
    "        x = residual_block_SERN_AwGOP(x, f)\n",
    "        x = residual_block_SERN_AwGOP(x, f)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    if num_classes == 2:\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    else:\n",
    "        outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_without_se(input_shape, num_classes, filters):\n",
    "    # Similar to create_SERN_AwGOP but without squeeze_and_excitation_block\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(filters[0], 5, strides=2, padding='same', activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "\n",
    "    for f in filters[1:]:\n",
    "        x = residual_block_without_se(x, f, downsample=True)\n",
    "        x = residual_block_without_se(x, f)\n",
    "        x = residual_block_without_se(x, f)\n",
    "        x = residual_block_without_se(x, f)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    if num_classes == 2:\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    else:\n",
    "        outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "def residual_block_without_se(x, filters, kernel_size=5, downsample=False):\n",
    "    y = branched_nodal_operator(x, filters, kernel_size)\n",
    "    y = branched_nodal_operator(y, filters, kernel_size)\n",
    "\n",
    "    if downsample:\n",
    "        x = downsample_block(x, filters)\n",
    "\n",
    "    attention_weights = layers.Dense(1, activation='sigmoid')(x)\n",
    "    gop_out = layers.Multiply()([attention_weights, y])\n",
    "    gop_out = layers.Add()([gop_out, x])\n",
    "    gop_out = layers.Activation('relu')(gop_out)\n",
    "    return gop_out\n",
    "\n",
    "def create_model_without_attention(input_shape, num_classes, filters):\n",
    "    # Similar to create_SERN_AwGOP but without attention mechanism\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(filters[0], 5, strides=2, padding='same', activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "\n",
    "    for f in filters[1:]:\n",
    "        x = residual_block_without_attention(x, f, downsample=True)\n",
    "        x = residual_block_without_attention(x, f)\n",
    "        x = residual_block_without_attention(x, f)\n",
    "        x = residual_block_without_attention(x, f)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    if num_classes == 2:\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    else:\n",
    "        outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "def residual_block_without_attention(x, filters, kernel_size=5, downsample=False):\n",
    "    y = branched_nodal_operator(x, filters, kernel_size)\n",
    "    y = branched_nodal_operator(y, filters, kernel_size)\n",
    "\n",
    "    if downsample:\n",
    "        x = downsample_block(x, filters)\n",
    "\n",
    "    y = squeeze_and_excitation_block(y)\n",
    "    y = layers.Add()([y, x])\n",
    "    y = layers.Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "def create_model_simple_conv(input_shape, num_classes, filters):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(filters[0], 5, strides=2, padding='same', activation='relu')(inputs)\n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "\n",
    "    for f in filters[1:]:\n",
    "        x = layers.Conv1D(f, 3, padding='same', activation='relu')(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2, strides=2, padding='same')(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    if num_classes == 2:\n",
    "        outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    else:\n",
    "        outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def analyze_computational_complexity(model):\n",
    "    total_params = model.count_params()\n",
    "    total_flops = 0\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, layers.Conv1D):\n",
    "            output_shape = layer.output_shape\n",
    "            kernel_size = layer.kernel_size[0]\n",
    "            input_channels = layer.input_shape[-1]\n",
    "            output_channels = layer.filters\n",
    "            flops = output_shape[1] * output_shape[2] * kernel_size * input_channels * output_channels\n",
    "            total_flops += flops\n",
    "        elif isinstance(layer, layers.Dense):\n",
    "            input_shape = layer.input_shape\n",
    "            output_shape = layer.output_shape\n",
    "            flops = input_shape[-1] * output_shape[-1]\n",
    "            total_flops += flops\n",
    "\n",
    "    return {\n",
    "        'total_params': total_params,\n",
    "        'total_flops': total_flops\n",
    "    }\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val):\n",
    "    if model.output_shape[-1] == 1:  # Binary classification\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:  # Multi-class classification\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, num_classes):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1) if num_classes > 2 else (y_pred > 0.5).astype(int).flatten()\n",
    "\n",
    "    results = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred_classes),\n",
    "        'precision': precision_score(y_test, y_pred_classes, average='weighted'),\n",
    "        'recall': recall_score(y_test, y_pred_classes, average='weighted'),\n",
    "        'f1': f1_score(y_test, y_pred_classes, average='weighted'),\n",
    "    }\n",
    "\n",
    "    if num_classes == 2:\n",
    "        results['auc_roc'] = roc_auc_score(y_test, y_pred)\n",
    "        results['average_precision'] = average_precision_score(y_test, y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "    return results, cm\n",
    "\n",
    "def perform_cross_validation(dataset, model_fn, filters):\n",
    "    data = np.load(dataset['data'])\n",
    "    labels = np.load(dataset['labels'])\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "\n",
    "    # Calculate and save overall dataset statistics\n",
    "    overall_stats = calculate_dataset_stats(data, labels)\n",
    "    save_results(overall_stats, f\"{dataset['name']}_overall_stats\")\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(data)):\n",
    "        logging.info(f\"Processing fold {fold + 1} for dataset {dataset['name']}\")\n",
    "\n",
    "        X_train, X_val = data[train_index], data[val_index]\n",
    "        y_train, y_val = labels[train_index], labels[val_index]\n",
    "\n",
    "        # Calculate and save train/val set statistics\n",
    "        train_stats = calculate_dataset_stats(X_train, y_train)\n",
    "        val_stats = calculate_dataset_stats(X_val, y_val)\n",
    "        save_results(train_stats, f\"{dataset['name']}_train_stats_fold_{fold}\")\n",
    "        save_results(val_stats, f\"{dataset['name']}_val_stats_fold_{fold}\")\n",
    "\n",
    "        model = model_fn(dataset['input_shape'], dataset['num_classes'], filters)\n",
    "        history = train_model(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "        results, cm = evaluate_model(model, X_val, y_val, dataset['num_classes'])\n",
    "        fold_results.append(results)\n",
    "\n",
    "        save_results(results, dataset['name'], fold)\n",
    "        save_confusion_matrix(cm, dataset['name'], fold)\n",
    "\n",
    "        complexity_results = analyze_computational_complexity(model)\n",
    "        save_results(complexity_results, f\"{dataset['name']}_complexity\", fold)\n",
    "\n",
    "    return calculate_average_results(fold_results)\n",
    "\n",
    "def calculate_average_results(fold_results):\n",
    "    avg_results = {}\n",
    "    for key in fold_results[0].keys():\n",
    "        avg_results[key] = np.mean([fold[key] for fold in fold_results])\n",
    "        avg_results[f'{key}_std'] = np.std([fold[key] for fold in fold_results])\n",
    "    return avg_results\n",
    "\n",
    "def analyze_model_components(dataset, model_fn, filters):\n",
    "    data = np.load(dataset['data'])\n",
    "    labels = np.load(dataset['labels'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    base_model = model_fn(dataset['input_shape'], dataset['num_classes'], filters)\n",
    "    base_results, _ = evaluate_model(base_model, X_test, y_test, dataset['num_classes'])\n",
    "\n",
    "    component_results = {'base_model': base_results}\n",
    "\n",
    "    # Analyze without SE blocks\n",
    "    model_without_se = create_model_without_se(dataset['input_shape'], dataset['num_classes'], filters)\n",
    "    results_without_se, _ = evaluate_model(model_without_se, X_test, y_test, dataset['num_classes'])\n",
    "    component_results['without_se'] = results_without_se\n",
    "\n",
    "    # Analyze without attention mechanism\n",
    "    model_without_attention = create_model_without_attention(dataset['input_shape'], dataset['num_classes'], filters)\n",
    "    results_without_attention, _ = evaluate_model(model_without_attention, X_test, y_test, dataset['num_classes'])\n",
    "    component_results['without_attention'] = results_without_attention\n",
    "\n",
    "    # Analyze with simpler convolution blocks\n",
    "    model_simple_conv = create_model_simple_conv(dataset['input_shape'], dataset['num_classes'], filters)\n",
    "    results_simple_conv, _ = evaluate_model(model_simple_conv, X_test, y_test, dataset['num_classes'])\n",
    "    component_results['simple_conv'] = results_simple_conv\n",
    "\n",
    "    save_results(component_results, f\"{dataset['name']}_component_analysis\")\n",
    "    return component_results\n",
    "\n",
    "def main():\n",
    "    for dataset in datasets:\n",
    "        logging.info(f\"Processing dataset: {dataset['name']}\")\n",
    "        \n",
    "        for filters in filter_combinations:\n",
    "            logging.info(f\"Using filter combination: {filters}\")\n",
    "            \n",
    "            try:\n",
    "                # Perform cross-validation\n",
    "                avg_results = perform_cross_validation(dataset, create_SERN_AwGOP, filters)\n",
    "                save_results(avg_results, f\"{dataset['name']}_average_results\")\n",
    "                \n",
    "                # Analyze model components\n",
    "                component_results = analyze_model_components(dataset, create_SERN_AwGOP, filters)\n",
    "                logging.info(f\"Component analysis results: {component_results}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"An error occurred while processing {dataset['name']}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SERN-AwGOP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
